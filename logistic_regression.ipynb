{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Machine Learned Sentiment Analysis using Python\n",
    "\n",
    "“Can our project correctly classify the sentiment of just about any\n",
    "sentence in the English language?”\n",
    "\n",
    "Our goal is to build and train a model that will be able to classify more\n",
    "than one dataset with over 70% accuracy on each of them. To this end,\n",
    "we might have to adjust some properties to avoid overfitting to one of the\n",
    "datasets to have it perform better on a general scale. Because we are\n",
    "still beginners to machine learning, we are more focused on getting a\n",
    "model successfully running.\n",
    "\n",
    "\n",
    "## 1.0 Binary classification using binary logistic regression\n",
    "\n",
    "For the first part of this notebook we will build a binary classification\n",
    "model, evaluated using logistic regression, as we learned in class. This\n",
    "section of the project is strongly based off of this blog post by\n",
    "Atharva Mashalkar.\n",
    "https://towardsdatascience.com/sentiment-analysis-using-logistic-regression-and-naive-bayes-16b806eb4c4b\n",
    "\n",
    "**Run the code cell below** to import the required packages.\n",
    "\n",
    "To start with, we will import the libraries by nltk (Natural Language Toolkit) package, which contains 5000\n",
    "which will help us with the preprocessing and training of our model. We will\n",
    "also need some libraries such as regular expressions to filter out\n",
    "unnecessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import sklearn.model_selection   # for train and test splits\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Preprocessing the data\n",
    "**Stop-Word Removal** : In English, words like a, an, the, as, in, on,\n",
    "etc. are considered as stop-words so according to our requirements\n",
    "we can remove them to reduce vocabulary size as these words don't\n",
    "contribute to the final meaning or sentiment of a sentence.\n",
    "To do this, we import the stopwords list from nltk. We also tokenize\n",
    "each sentence  string into a list of words after cleaning up filler words\n",
    "such as retweet, hashtags, and URLs.\n",
    "\n",
    "Additionally, we need to convert all letters in the string to lowercase.\n",
    "This helps reduce unnecessary bloat in the dataset.\n",
    "\n",
    "Next, we tokenize the strings: separate all the words in the string into\n",
    "a python list of words.\n",
    "\n",
    "As a final step, we perform stemming: **stemming** refers to the process of\n",
    "removing suffixes and reducing a word to some base form such that all\n",
    "different variants of that word can be represented by the same form\n",
    "(e.g., “walk” and “walking” are both reduced to “walk”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Preprocessing tweets\n",
    "def process_tweet(tweet):\n",
    "    #Remove old style retweet text \"RT\"\n",
    "    cleaned_tweet = re.sub(r'^RT[\\s]','', tweet)\n",
    "\n",
    "    #Remove URLS\n",
    "    cleaned_tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*','', cleaned_tweet)\n",
    "\n",
    "    #Remove hashtags\n",
    "    cleaned_tweet = re.sub(r'#','',cleaned_tweet)\n",
    "\n",
    "    #convert tweet to lowercase\n",
    "    cleaned_tweet = cleaned_tweet.lower()\n",
    "\n",
    "    #Instantiate tokenizer class\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,    strip_handles=True, reduce_len=True)\n",
    "\n",
    "    #Tokenize tweets\n",
    "    tweet_tokens = tokenizer.tokenize(cleaned_tweet)\n",
    "\n",
    "    #Import the english stop words list from nltk\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    #Creating a list of words without stopwords\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if word not in stopwords_english and word not in string.punctuation:\n",
    "            tweets_clean.append(word)\n",
    "\n",
    "    #Instantiate stemming class\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    #Creating a list of stems of words in tweet\n",
    "    tweets_stem = []\n",
    "    for word in tweets_clean:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        tweets_stem.append(stem_word)\n",
    "\n",
    "    return tweets_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Building the Frequency Dictionary\n",
    "\n",
    "Here we define a function that will take as input tweets and their labels\n",
    "as parameters. It will go through every tweet, preprocess them with the\n",
    "function we just defined, count the occurrence of every word in the data\n",
    "set and create a frequency dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Frequency generating function\n",
    "def build_freqs(tweets, ys):\n",
    "    yslist = np.squeeze(ys).tolist() #squeeze is needed or the list\n",
    "                                     # will end up with one element\n",
    "\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            freqs[pair] = freqs.get(pair, 0) + 1\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a function that will extract features from a tweet\n",
    "through the use of the ‘freqs’ dictionary and the defined process_tweet\n",
    "function from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3))\n",
    "\n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1\n",
    "\n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "\n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word,1),0)\n",
    "\n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word,0),0)\n",
    "\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing, we are importing the sample tweets from nltk and splitting\n",
    "the data into training sets and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import dataset from nltk\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "#create labels\n",
    "one_array = [1] * len(all_positive_tweets)\n",
    "zero_array = [0] * len(all_negative_tweets)\n",
    "\n",
    "all_tweets = all_positive_tweets + all_negative_tweets\n",
    "all_labels = one_array + zero_array\n",
    "tweets_and_labels = np.vstack((all_tweets,all_labels)).T\n",
    "\n",
    "#extract X (data) and y (labels) columns\n",
    "nltk_X = tweets_and_labels[:,0].tolist()\n",
    "nltk_y = tweets_and_labels[:,1].T.astype(int).tolist()\n",
    "\n",
    "#split the data set into train and test sets with a 80/20 ratio\n",
    "nltk_X_train, nltk_X_test, nltk_y_train, nltk_y_test = sklearn.model_selection.train_test_split(nltk_X, nltk_y, test_size=0.20, random_state=0)\n",
    "nltk_y_train = np.array([nltk_y_train]).T\n",
    "nltk_y_test = np.array([nltk_y_test]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With our required functions for processing tweets ready to go, we can begin\n",
    "to build our logistic regression model.\n",
    "\n",
    "The logistic regression maps the input $\\mathbf{x}_i$ into the following\n",
    "output:\n",
    "\n",
    "$p(y_i = 1 \\mid \\mathbf{x}_i, \\mathbf{w}) = \\sigma(\\mathbf{w}^T\\mathbf{x}_i) =  \\sigma(w_0 + w_1 x_1 + w_2 x_2)$.\n",
    "\n",
    "$\\sigma$ is the sigmoid function, that is defined as:\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1 + e^{-z}} = (1+e^{-z})^{-1}$\n",
    "\n",
    "The output of the sigmoid function is a value between 0 and 1. Let us\n",
    "define the sigmoid function to be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Returns the element-wise logistic sigmoid of z.\"\"\"\n",
    "    # Your code here. Aim for 1 line.\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Cost Function and Gradient Descent\n",
    "\n",
    "Our goal is to find a configuration of our parameters $\\mathbf{w}$ that\n",
    "minimizes our objective function (BCE). For logistic regression, we use\n",
    "the binary cross-entropy loss. Recall from Lecture 1 that the basic\n",
    "logistic regression training objective (learning objective) is:\n",
    "\n",
    "$$\n",
    "\\ell_\\text{LR}(\\mathbf{w}) = \\sum_{i=1}^N y_i \\ln \\sigma(\\mathbf{w}^T \\mathbf{x}_i) + (1-y_i) \\ln \\left(1-\\sigma(\\mathbf{w}^T \\mathbf{x}_i)\\right)\n",
    "$$\n",
    "\n",
    "The \"basic\" gradient for the above training objective is on a slide\n",
    "titled \"Maximum likelihood estimate for LR\" from Lecture 1, and\n",
    "reproduced here:\n",
    "\n",
    "$$\n",
    "\\nabla \\ell_\\text{LR}(\\mathbf{w}) = \\sum_{i=1}^N (\\sigma(\\mathbf{w}^T \\mathbf{x}_i) - y_i)\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "Let's define a few functions that implement these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w, lr, num_iters):\n",
    "\n",
    "    m = len(x)\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "\n",
    "        z = np.dot(x,w)\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        # calculate the cost function\n",
    "        J = (-1/m)*(np.dot(y.T,np.log(h)) + np.dot((1-y).T,np.log(1-h)))\n",
    "\n",
    "        # update the weights by gradient descent\n",
    "        w = w - (lr/m)*np.dot(x.T, h-y)\n",
    "\n",
    "    J = float(J)\n",
    "    return J, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4 Training and Testing our Model\n",
    "\n",
    "As all the required functions are ready we can finally train our model\n",
    "using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_18096/1421374834.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mnltk_X_gd\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnltk_X_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnltk_X_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[0mnltk_X_gd\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m=\u001B[0m \u001B[0mextract_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnltk_X_train\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnltk_freqs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;31m# training labels corresponding to X\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_18096/176290919.py\u001B[0m in \u001B[0;36mextract_features\u001B[1;34m(tweet, freqs)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mextract_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtweet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfreqs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[1;31m# process_tweet tokenizes, stems, and removes stopwords\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m     \u001B[0mword_l\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mprocess_tweet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtweet\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;31m# 3 elements in the form of a 1 x 3 vector\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_18096/2014791697.py\u001B[0m in \u001B[0;36mprocess_tweet\u001B[1;34m(tweet)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[1;31m#Tokenize tweets\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m     \u001B[0mtweet_tokens\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcleaned_tweet\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m     \u001B[1;31m#Import the english stop words list from nltk\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\casual.py\u001B[0m in \u001B[0;36mtokenize\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m    346\u001B[0m         \u001B[1;31m# Normalize word lengthening\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    347\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreduce_len\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 348\u001B[1;33m             \u001B[0mtext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mreduce_lengthening\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    349\u001B[0m         \u001B[1;31m# Shorten problematic sequences of characters\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    350\u001B[0m         \u001B[0msafe_text\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mHANG_RE\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msub\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mr\"\\1\\1\\1\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tokenize\\casual.py\u001B[0m in \u001B[0;36mreduce_lengthening\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m    395\u001B[0m     \"\"\"\n\u001B[0;32m    396\u001B[0m     \u001B[0mpattern\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mregex\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mr\"(.)\\1{2,}\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 397\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mpattern\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msub\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mr\"\\1\\1\\1\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    398\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    399\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "nltk_freqs = build_freqs(nltk_X_train, nltk_y_train)\n",
    "\n",
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "nltk_X_gd = np.zeros((len(nltk_X_train), 3))\n",
    "for i in range(len(nltk_X_train)):\n",
    "    nltk_X_gd[i, :]= extract_features(nltk_X_train[i], nltk_freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "nltk_Y_gd = nltk_y_train\n",
    "\n",
    "# Apply gradient descent to extract the weight vector w\n",
    "nltk_J, nltk_w = gradient_descent(nltk_X_gd, nltk_Y_gd, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {nltk_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(nltk_w)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Right above here, we receive and print the final cost J as well as the\n",
    "weight matrix w after all the training is done. This weight matrix\n",
    "constitutes our binary classification based on logistic regression model.\n",
    "\n",
    "Let's proceed to write two more functions which when given a tweet, will\n",
    "predict results using the freqs dictionary and weights matrix. The second\n",
    "function will use the predict function and provide the accuracy of\n",
    "the model on the given testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        freqs: a dictionary of the frequencies of each tuple (word, label)\n",
    "        theta: vector of weights\n",
    "    Output:\n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    \"\"\"\n",
    "\n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "\n",
    "    # make the prediction using x and theta\n",
    "    z = np.dot(x,theta)\n",
    "    y_pred = sigmoid(z)\n",
    "    return y_pred\n",
    "\n",
    "def test_logistic_regression(X_test, y_test, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X_test: a list of tweets\n",
    "        y_test: vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary of the frequencies of each tuple (word, label)\n",
    "        theta: vector of weights\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "\n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "\n",
    "    for tweet in X_test:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "\n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "    # With the above implementation, y_hat is a list, but test_y is a\n",
    "    # (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them\n",
    "    # using the '==' operator\n",
    "    y_hat = np.array(y_hat)\n",
    "    test_y = y_test.reshape(-1)\n",
    "    accuracy = np.sum((test_y == y_hat).astype(int))/len(X_test)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With all the required functions defined, we can proceed to try out our\n",
    "model and look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acc = test_logistic_regression(nltk_X_test, nltk_y_test, nltk_freqs, nltk_w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have obtained a pretty high accuracy (around 99.0%) with the trained\n",
    "model. Let's formulate a few sentences and see what the model predicts\n",
    "out of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tweet = \"I hate insta so much everyone is nicer on here\"\n",
    "prediction = predict_tweet(tweet, nltk_freqs, nltk_w)\n",
    "print(\"This tweet is positive\" if prediction >= 0.5 else \"This tweet is negative\")\n",
    "\n",
    "tweet = \"Everyday is my favorite, waking up one more day makes each day awesome\"\n",
    "prediction = predict_tweet(tweet, nltk_freqs, nltk_w)\n",
    "print(\"This tweet is positive\" if prediction >= 0.5 else \"This tweet is negative\")\n",
    "\n",
    "tweet = \"Checking the box full of ole' photos is always fun!\"\n",
    "prediction = predict_tweet(tweet, nltk_freqs, nltk_w)\n",
    "print(\"This tweet is positive\" if prediction >= 0.5 else \"This tweet is negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Expanding the data set\n",
    "\n",
    "Let's continue exploring the model by training it using another popular data\n",
    "set. The data set in question is \"Twitter US Airline Sentiment\" (retrieved\n",
    "from https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment?resource=download ).\n",
    "For the ease of use we have included the data set already\n",
    "within the repository. Let's do a few lines of code to import and preview\n",
    "the data in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Tweets.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Unlike our previous data set, this one contains more columns which we might\n",
    "not need. It is our job to process the data to make it fit the model\n",
    "defined above. However, do observe that there is one more sentiment in the\n",
    "labels: the neutral label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df.airline_sentiment.value_counts())\n",
    "plt.figure(figsize = (10, 8))\n",
    "ax = sns.countplot(x = 'airline_sentiment', data = df, palette = 'pastel')\n",
    "ax.set_title(label = 'Total number of sentiments of tweets', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's define a few functions that will help us with the text processing.\n",
    "Remember, currently we are working with binary classification of positive\n",
    "and negative sentiments. For this reason, we will be discarding all neutral\n",
    "labels and their corresponding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#remove all neutral-labeled data\n",
    "df =  df[df.airline_sentiment != \"neutral\"]\n",
    "print(df.airline_sentiment.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we should convert the sentiments into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert sentiments to 0 or 1\n",
    "def convert_sentiment(sentiment):\n",
    "    if  sentiment == \"positive\":\n",
    "        return 1\n",
    "    elif sentiment == \"negative\":\n",
    "        return 0\n",
    "\n",
    "df.airline_sentiment = df.airline_sentiment.apply(lambda x : convert_sentiment(x))\n",
    "print(df.airline_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One final step, we will extract each tweet text from the data and their\n",
    "respective labels. We also need to split the data into training and test\n",
    "sets as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#extract columns in the csv file as rows\n",
    "airline_X = df['text'].tolist()\n",
    "airline_y = df['airline_sentiment'].tolist()\n",
    "\n",
    "#split into 80:20 train to test ratio\n",
    "airline_X_train, airline_X_test, airline_y_train, airline_y_test = sklearn.model_selection.train_test_split(airline_X, airline_y, test_size=0.20, random_state=0)\n",
    "airline_y_train = np.array([airline_y_train]).T.astype(float)\n",
    "airline_y_test = np.array([airline_y_test]).T.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data has now undergone transformation and is ready to be handled by\n",
    "our previously defined functions. We simply have to repeat the code\n",
    "sections written out before but now using the airline data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "airline_freqs = build_freqs(airline_X_train, airline_y_train)\n",
    "\n",
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "airline_X_gd = np.zeros((len(airline_X_train), 3))\n",
    "for i in range(len(airline_X_train)):\n",
    "    airline_X_gd[i, :]= extract_features(airline_X_train[i], airline_freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "airline_Y_gd = airline_y_train\n",
    "\n",
    "# Apply gradient descent to extract the weight vector w\n",
    "airline_J, airline_w = gradient_descent(airline_X_gd, airline_Y_gd, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {airline_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(airline_w)]}\")\n",
    "\n",
    "airline_acc = test_logistic_regression(airline_X_test, airline_y_test, airline_freqs, airline_w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(airline_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The resulting accuracy of the model trained on the airline sentiment\n",
    "data seems lower than the model trained on nltk's tweets sentiments.\n",
    "This is to be discussed in our report submission.\n",
    "\n",
    "### 1.7 Combining nltk's Dataset with Twitter US Airline Sentiment Analysis's Dataset\n",
    "\n",
    "As a final experiment, I would like to repeat the training and testing\n",
    "of our model, but this time with both data sets combined. Let's do a\n",
    "little processing of the data to achieve this objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#combine both list of tweets into one data set\n",
    "combined_X = airline_X + nltk_X\n",
    "combined_y = airline_y + nltk_y\n",
    "\n",
    "#split into 80:20 train to test ratio\n",
    "combined_X_train, combined_X_test, combined_y_train, combined_y_test = sklearn.model_selection.train_test_split(combined_X, combined_y, test_size=0.20, random_state=0)\n",
    "combined_y_train = np.array([combined_y_train]).T.astype(float)\n",
    "combined_y_test = np.array([combined_y_test]).T.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data is now ready to go through the training loops and testing outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "combined_freqs = build_freqs(combined_X_train, combined_y_train)\n",
    "\n",
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "combined_X_gd = np.zeros((len(combined_X_train), 3))\n",
    "for i in range(len(combined_X_train)):\n",
    "    combined_X_gd[i, :]= extract_features(combined_X_train[i], combined_freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "combined_Y_gd = combined_y_train\n",
    "\n",
    "# Apply gradient descent to extract the weight vector w\n",
    "combined_J, combined_w = gradient_descent(combined_X_gd, combined_Y_gd, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {combined_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(combined_w)]}\")\n",
    "\n",
    "combined_acc = test_logistic_regression(combined_X_test, combined_y_test, combined_freqs, combined_w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(combined_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The accuracy on this model appears to be lower than that trained solely\n",
    "on nltk's sentiment data set, but shows to be higher than that trained\n",
    "on the airline sentiment data set. We will discuss this in detail in the\n",
    "written reports as well.\n",
    "\n",
    "### 1.8 Training the model by carrying over the weight vector\n",
    "\n",
    "In our previous three experiments, we conducted the training on\n",
    "a clean slate by initializing the weights vectors to zero. What\n",
    "would happen if we were to train our binary classification model\n",
    "on one data set and pass the resulting weight vector as the\n",
    "initial weight vector for the second data set? Let's take a\n",
    "look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Apply gradient descent to extract the weight vector w\n",
    "shared_w_J, shared_w_w = gradient_descent(airline_X_gd, airline_Y_gd, nltk_w, 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {shared_w_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(shared_w_w)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Because the model has already been trained on both data sets,\n",
    "we are testing it against the combined set's test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shared_w_acc = test_logistic_regression(combined_X_test, combined_y_test, combined_freqs, shared_w_w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(shared_w_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The results are close enough to that of the combined data set's model's\n",
    "results. As usual, we will discuss the significance of this output\n",
    "inside the written report.\n",
    "\n",
    "### 1.9 Introducing one more dataset: Sentiment140\n",
    "\n",
    "We will perform sections 1.6 to 1.10 again with an additional dataset.\n",
    "The sentiment140 dataset contains 1,600,000 tweets extracted using the\n",
    "Twitter API (retrieved from https://www.kaggle.com/datasets/kazanova/sentiment140 ).\n",
    "This means we are dealing with tweets once more! The tweets\n",
    "have been labeled (0 for negative, 2 for neutral, 4 for positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_sentiment140 = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)\n",
    "df_sentiment140.columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "print(df_sentiment140.shape)\n",
    "df_sentiment140.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This dataset is actually over 200 MB in file size and way too large to\n",
    "comfortably work with on a low-end computer. We will only work with\n",
    "about half of the data, shuffled with a constant random_state variable so\n",
    "that it is reproducible every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_sentiment140 = shuffle(df_sentiment140,random_state=0)\n",
    "df_sentiment140 = df_sentiment140[1:500000]\n",
    "df_sentiment140['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the airline sentiment data set, this one contains some columns which\n",
    "we might not need. We will once again have to preprocess the data. The\n",
    "neutral label is also present and will have to be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(df_sentiment140.target.value_counts())\n",
    "plt.figure(figsize = (10, 8))\n",
    "ax = sns.countplot(x = 'target', data = df_sentiment140, palette = 'pastel')\n",
    "ax.set_title(label = 'Total number of sentiments of tweets', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#remove all neutral-labeled data\n",
    "df_sentiment140 =  df_sentiment140[df_sentiment140.target != 2]\n",
    "print(df_sentiment140.target.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should convert the sentiments into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert sentiments to 0 or 1\n",
    "def convert_sentiment_140(sentiment):\n",
    "    if  sentiment == 4:\n",
    "        return 1\n",
    "    elif sentiment == 0:\n",
    "        return 0\n",
    "\n",
    "df_sentiment140.target = df_sentiment140.target.apply(lambda x : convert_sentiment_140(x))\n",
    "print(df_sentiment140.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we need to extract the data and their corresponding labels. \n",
    "We also need to split the data into training and test sets as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#extract columns in the csv file as rows\n",
    "sentiment140_X = df_sentiment140['text'].tolist()\n",
    "sentiment140_y = df_sentiment140['target'].tolist()\n",
    "\n",
    "#split into 80:20 train to test ratio\n",
    "sentiment140_X_train, sentiment140_X_test, sentiment140_y_train, sentiment140_y_test = sklearn.model_selection.train_test_split(sentiment140_X, sentiment140_y, test_size=0.20, random_state=0)\n",
    "sentiment140_y_train = np.array([sentiment140_y_train]).T.astype(float)\n",
    "sentiment140_y_test = np.array([sentiment140_y_test]).T.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been successfully extracted and is now ready to be handled by\n",
    "the simple logistic regression model. We simply have to repeat the code\n",
    "sections written out before but now using the sentiment140 data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentiment140_freqs = build_freqs(sentiment140_X_train, sentiment140_y_train)\n",
    "\n",
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "sentiment140_X_gd = np.zeros((len(sentiment140_X_train), 3))\n",
    "for i in range(len(sentiment140_X_train)):\n",
    "    sentiment140_X_gd[i, :]= extract_features(sentiment140_X_train[i], sentiment140_freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "sentiment140_Y_gd = sentiment140_y_train\n",
    "\n",
    "# Apply gradient descent to extract the weight vector w\n",
    "sentiment140_J, sentiment140_w = gradient_descent(sentiment140_X_gd, sentiment140_Y_gd, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {sentiment140_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(sentiment140_w)]}\")\n",
    "\n",
    "sentiment140_acc = test_logistic_regression(sentiment140_X_test, sentiment140_y_test, sentiment140_freqs, sentiment140_w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(sentiment140_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the results of the model trained on the sentiment140 dataset,\n",
    "we can observe that it is even lower than that of the airline sentiment\n",
    "dataset's! This is to be discussed in our report submission.\n",
    "\n",
    "### 1.10 Training the model using all three datasets\n",
    "\n",
    "One final experiment to end this section: we will carry over the weights\n",
    "obtained from training the model on sentiment140's dataset and continue\n",
    "the training with the combined dataset of nltk and airline sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply gradient descent to extract the weight vector w\n",
    "reshared_w_J, reshared_w_w = gradient_descent(combined_X_gd, combined_Y_gd, sentiment140_w, 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {reshared_w_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(reshared_w_w)]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_freqs.update(sentiment140_freqs)\n",
    "\n",
    "reshared_w_acc = test_logistic_regression(combined_X_test, combined_y_test, combined_freqs, reshared_w_w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(reshared_w_acc*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With one quick look at the accuracy, we can see that it has greatly\n",
    "improved. The reasons for this will be discussed within the report.\n",
    "This concludes all our experiments and the section for sentiment analysis\n",
    "by binary classification using logistic regression."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Comp-472-Mini-Project)",
   "language": "python",
   "name": "pycharm-27aa6ddb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}