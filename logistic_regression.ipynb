{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Machine Learned Sentiment Analysis using Python\n",
    "\n",
    "“Can our project correctly classify the sentiment of just about any\n",
    "sentence in the English language?”\n",
    "\n",
    "Our goal is to build and train a model that will be able to classify more\n",
    "than one dataset with over 70% accuracy on each of them. To this end,\n",
    "we might have to adjust some properties to avoid overfitting to one of the\n",
    "datasets to have it perform better on a general scale. Because we are\n",
    "still beginners to machine learning, we are more focused on getting a\n",
    "model successfully running.\n",
    "\n",
    "\n",
    "## Binary classification using binary logistic regression\n",
    "\n",
    "For the first part of this notebook we will build a binary classification\n",
    "model, evaluated using logistic regression, as we learned in class.\n",
    "\n",
    "**Run the code cell below** to import the required packages.\n",
    "\n",
    "To start with, we will import the libraries by nltk (Natural Language Toolkit) package, which contains 5000\n",
    "which will help us with the preprocessing and training of our model. We will\n",
    "also need some libraries such as regular expressions to filter out\n",
    "unnecessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import twitter_samples\n",
    "import sklearn.model_selection   # for train and test splits\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Preprocessing the data\n",
    "**Stop-Word Removal** : In English, words like a, an, the, as, in, on,\n",
    "etc. are considered as stop-words so according to our requirements\n",
    "we can remove them to reduce vocabulary size as these words don't\n",
    "contribute to the final meaning or sentiment of a sentence.\n",
    "To do this, we import the stopwords list from nltk. We also tokenize\n",
    "each sentence  string into a list of words after cleaning up filler words\n",
    "such as retweet, hashtags, and URLs.\n",
    "\n",
    "As a final step, we perform stemming: **stemming** refers to the process of\n",
    "removing suffixes and reducing a word to some base form such that all\n",
    "different variants of that word can be represented by the same form\n",
    "(e.g., “walk” and “walking” are both reduced to “walk”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Preprocessing tweets\n",
    "def process_tweet(tweet):\n",
    "    #Remove old style retweet text \"RT\"\n",
    "    cleaned_tweet = re.sub(r'^RT[\\s]','', tweet)\n",
    "\n",
    "    #Remove URLS\n",
    "    cleaned_tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*','', cleaned_tweet)\n",
    "\n",
    "    #Remove hashtags\n",
    "    cleaned_tweet = re.sub(r'#','',cleaned_tweet)\n",
    "\n",
    "    #Instantiate tokenizer class\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,    strip_handles=True, reduce_len=True)\n",
    "\n",
    "    #Tokenize tweets\n",
    "    tweet_tokens = tokenizer.tokenize(cleaned_tweet)\n",
    "\n",
    "    #Import the english stop words list from nltk\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    #Creating a list of words without stopwords\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if word not in stopwords_english and word not in string.punctuation:\n",
    "            tweets_clean.append(word)\n",
    "\n",
    "    #Instantiate stemming class\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    #Creating a list of stems of words in tweet\n",
    "    tweets_stem = []\n",
    "    for word in tweets_clean:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        tweets_stem.append(stem_word)\n",
    "\n",
    "    return tweets_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Building the Frequency Dictionary\n",
    "\n",
    "Here we define a function that will take as input tweets and their labels\n",
    "as parameters. It will go through every tweet, preprocess them with the\n",
    "function we just defined, count the occurrence of every word in the data\n",
    "set and create a frequency dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Frequency generating function\n",
    "def build_freqs(tweets, ys):\n",
    "    yslist = np.squeeze(ys).tolist() #squeeze is needed or the list\n",
    "                                     # will end up with one element\n",
    "\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            freqs[pair] = freqs.get(pair, 0) + 1\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a function that will extract features from a tweet\n",
    "through the use of the ‘freqs’ dictionary and the defined process_tweet\n",
    "function from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3))\n",
    "\n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1\n",
    "\n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "\n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word,1),0)\n",
    "\n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word,0),0)\n",
    "\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing, we are importing the sample tweets from nltk and splitting\n",
    "the data into training sets and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import dataset from nltk\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "#create labels\n",
    "one_array = [1] * len(all_positive_tweets)\n",
    "zero_array = [0] * len(all_negative_tweets)\n",
    "\n",
    "all_tweets = all_positive_tweets + all_negative_tweets\n",
    "all_labels = one_array + zero_array\n",
    "tweets_and_labels = np.vstack((all_tweets,all_labels)).T\n",
    "\n",
    "#introduce randomization to the training and test sets\n",
    "np.random.shuffle(tweets_and_labels)\n",
    "\n",
    "#extract X (data) and y (labels) columns\n",
    "X = tweets_and_labels[:,0]\n",
    "y = tweets_and_labels[:,1].T\n",
    "\n",
    "#split the data set into train and test sets with a 80/20 ratio\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.20)\n",
    "X_train = X_train.tolist()\n",
    "X_test = X_test.tolist()\n",
    "y_train = np.array([y_train]).T.astype(float)\n",
    "y_test = np.array([y_test]).T.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With our required functions for processing tweets ready to go, we can begin\n",
    "to build our logistic regression model.\n",
    "\n",
    "The logistic regression maps the input $\\mathbf{x}_i$ into the following\n",
    "output:\n",
    "\n",
    "$p(y_i = 1 \\mid \\mathbf{x}_i, \\mathbf{w}) = \\sigma(\\mathbf{w}^T\\mathbf{x}_i) =  \\sigma(w_0 + w_1 x_1 + w_2 x_2)$.\n",
    "\n",
    "$\\sigma$ is the sigmoid function, that is defined as:\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1 + e^{-z}} = (1+e^{-z})^{-1}$\n",
    "\n",
    "The output of the sigmoid function is a value between 0 and 1. Let us\n",
    "define the sigmoid function to be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Returns the element-wise logistic sigmoid of z.\"\"\"\n",
    "    # Your code here. Aim for 1 line.\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function and Gradient Descent\n",
    "\n",
    "Our goal is to find a configuration of our parameters $\\mathbf{w}$ that\n",
    "minimizes our objective function (BCE). For logistic regression, we use\n",
    "the binary cross-entropy loss. Recall from Lecture 1 that the basic\n",
    "logistic regression training objective (learning objective) is:\n",
    "\n",
    "$$\n",
    "\\ell_\\text{LR}(\\mathbf{w}) = \\sum_{i=1}^N y_i \\ln \\sigma(\\mathbf{w}^T \\mathbf{x}_i) + (1-y_i) \\ln \\left(1-\\sigma(\\mathbf{w}^T \\mathbf{x}_i)\\right)\n",
    "$$\n",
    "\n",
    "The \"basic\" gradient for the above training objective is on a slide\n",
    "titled \"Maximum likelihood estimate for LR\" from Lecture 1, and\n",
    "reproduced here:\n",
    "\n",
    "$$\n",
    "\\nabla \\ell_\\text{LR}(\\mathbf{w}) = \\sum_{i=1}^N (\\sigma(\\mathbf{w}^T \\mathbf{x}_i) - y_i)\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "Let's define a few functions that implement these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w, lr, num_iters):\n",
    "\n",
    "    m = len(x)\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "\n",
    "        z = np.dot(x,w)\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        # calculate the cost function\n",
    "        J = (-1/m)*(np.dot(y.T,np.log(h)) + np.dot((1-y).T,np.log(1-h)))\n",
    "\n",
    "        # update the weights by gradient descent\n",
    "        w = w - (lr/m)*np.dot(x.T, h-y)\n",
    "\n",
    "    J = float(J)\n",
    "    return J, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As all the required functions are ready we can finally train our model\n",
    "using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.23869960.\n",
      "The resulting vector of weights is [7e-08, 0.00052556, -0.00055654]\n"
     ]
    }
   ],
   "source": [
    "freqs = build_freqs(X_train, y_train)\n",
    "\n",
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(X_train), 3))\n",
    "for i in range(len(X_train)):\n",
    "    X[i, :]= extract_features(X_train[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = y_train\n",
    "\n",
    "# Apply gradient descent to extract the weight vector w\n",
    "J, w = gradient_descent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(w)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Right above here, we receive and print the final cost J as well as the\n",
    "weight matrix w after all the training is done. This weight matrix\n",
    "constitutes our binary classification based on logistic regression model.\n",
    "\n",
    "Let's proceed to write two more functions which when given a tweet, will\n",
    "predict results using the freqs dictionary and weights matrix. The second\n",
    "function will use the predict function and provide the accuracy of\n",
    "the model on the given testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        freqs: a dictionary of the frequencies of each tuple (word, label)\n",
    "        theta: vector of weights\n",
    "    Output:\n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    \"\"\"\n",
    "\n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "\n",
    "    # make the prediction using x and theta\n",
    "    z = np.dot(x,theta)\n",
    "    y_pred = sigmoid(z)\n",
    "    return y_pred\n",
    "\n",
    "def test_logistic_regression(X_test, y_test, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X_test: a list of tweets\n",
    "        y_test: vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary of the frequencies of each tuple (word, label)\n",
    "        theta: vector of weights\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "\n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "\n",
    "    for tweet in X_test:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "\n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "    # With the above implementation, y_hat is a list, but test_y is a\n",
    "    # (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them\n",
    "    # using the '==' operator\n",
    "    y_hat = np.array(y_hat)\n",
    "    test_y = y_test.reshape(-1)\n",
    "    accuracy = np.sum((test_y == y_hat).astype(int))/len(X_test)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With all the required functions defined, we can proceed to try out our\n",
    "model and look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 99.15%\n"
     ]
    }
   ],
   "source": [
    "acc = test_logistic_regression(X_test, y_test, freqs, w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have obtained a pretty high accuracy (around 99.0%) with the trained\n",
    "model. Let's formulate a few sentences and see what the model predicts\n",
    "out of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tweet is negative\n",
      "This tweet is positive\n",
      "This tweet is positive\n"
     ]
    }
   ],
   "source": [
    "tweet = \"I hate insta so much everyone is nicer on here\"\n",
    "prediction = predict_tweet(tweet, freqs, w)\n",
    "print(\"This tweet is positive\" if prediction >= 0.5 else \"This tweet is negative\")\n",
    "\n",
    "tweet = \"Everyday is my favorite, waking up one more day makes each day awesome\"\n",
    "prediction = predict_tweet(tweet, freqs, w)\n",
    "print(\"This tweet is positive\" if prediction >= 0.5 else \"This tweet is negative\")\n",
    "\n",
    "tweet = \"Checking the box full of ole' photos is always fun!\"\n",
    "prediction = predict_tweet(tweet, freqs, w)\n",
    "print(\"This tweet is positive\" if prediction >= 0.5 else \"This tweet is negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Comp-472-Mini-Project)",
   "language": "python",
   "name": "pycharm-27aa6ddb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}