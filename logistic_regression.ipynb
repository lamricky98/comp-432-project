{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Machine Learned Sentiment Analysis using Python\n",
    "\n",
    "“Can our project correctly classify the sentiment of just about any\n",
    "sentence in the English language?”\n",
    "\n",
    "Our goal is to build and train a model that will be able to classify more\n",
    "than one dataset with over 70% accuracy on each of them. To this end,\n",
    "we might have to adjust some properties to avoid overfitting to one of the\n",
    "datasets to have it perform better on a general scale. Because we are\n",
    "still beginners to machine learning, we are more focused on getting a\n",
    "model successfully running.\n",
    "\n",
    "\n",
    "## 1.0 Binary classification using binary logistic regression\n",
    "\n",
    "For the first part of this notebook we will build a binary classification\n",
    "model, evaluated using logistic regression, as we learned in class. This\n",
    "section of the project is strongly based off of this blog post by\n",
    "Atharva Mashalkar.\n",
    "https://towardsdatascience.com/sentiment-analysis-using-logistic-regression-and-naive-bayes-16b806eb4c4b\n",
    "\n",
    "**Run the code cell below** to import the required packages.\n",
    "\n",
    "To start with, we will import the libraries by nltk (Natural Language Toolkit) package, which contains 5000\n",
    "which will help us with the preprocessing and training of our model. We will\n",
    "also need some libraries such as regular expressions to filter out\n",
    "unnecessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import sklearn.model_selection   # for train and test splits\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Preprocessing the data\n",
    "**Stop-Word Removal** : In English, words like a, an, the, as, in, on,\n",
    "etc. are considered as stop-words so according to our requirements\n",
    "we can remove them to reduce vocabulary size as these words don't\n",
    "contribute to the final meaning or sentiment of a sentence.\n",
    "To do this, we import the stopwords list from nltk. We also tokenize\n",
    "each sentence  string into a list of words after cleaning up filler words\n",
    "such as retweet, hashtags, and URLs.\n",
    "\n",
    "Additionally, we need to convert all letters in the string to lowercase.\n",
    "This helps reduce unnecessary bloat in the dataset.\n",
    "\n",
    "Next, we tokenize the strings: separate all the words in the string into\n",
    "a python list of words.\n",
    "\n",
    "As a final step, we perform stemming: **stemming** refers to the process of\n",
    "removing suffixes and reducing a word to some base form such that all\n",
    "different variants of that word can be represented by the same form\n",
    "(e.g., “walk” and “walking” are both reduced to “walk”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Preprocessing tweets\n",
    "def process_tweet(tweet):\n",
    "    #Remove old style retweet text \"RT\"\n",
    "    cleaned_tweet = re.sub(r'^RT[\\s]','', tweet)\n",
    "\n",
    "    #Remove URLS\n",
    "    cleaned_tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*','', cleaned_tweet)\n",
    "\n",
    "    #Remove hashtags\n",
    "    cleaned_tweet = re.sub(r'#','',cleaned_tweet)\n",
    "\n",
    "    #convert tweet to lowercase\n",
    "    cleaned_tweet = cleaned_tweet.lower()\n",
    "\n",
    "    #Instantiate tokenizer class\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,    strip_handles=True, reduce_len=True)\n",
    "\n",
    "    #Tokenize tweets\n",
    "    tweet_tokens = tokenizer.tokenize(cleaned_tweet)\n",
    "\n",
    "    #Import the english stop words list from nltk\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    #Creating a list of words without stopwords\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if word not in stopwords_english and word not in string.punctuation:\n",
    "            tweets_clean.append(word)\n",
    "\n",
    "    #Instantiate stemming class\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    #Creating a list of stems of words in tweet\n",
    "    tweets_stem = []\n",
    "    for word in tweets_clean:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        tweets_stem.append(stem_word)\n",
    "\n",
    "    return tweets_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Building the Frequency Dictionary\n",
    "\n",
    "Here we define a function that will take as input tweets and their labels\n",
    "as parameters. It will go through every tweet, preprocess them with the\n",
    "function we just defined, count the occurrence of every word in the data\n",
    "set and create a frequency dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Frequency generating function\n",
    "def build_freqs(tweets, ys):\n",
    "    yslist = np.squeeze(ys).tolist() #squeeze is needed or the list\n",
    "                                     # will end up with one element\n",
    "\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            freqs[pair] = freqs.get(pair, 0) + 1\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a function that will extract features from a tweet\n",
    "through the use of the ‘freqs’ dictionary and the defined process_tweet\n",
    "function from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "\n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3))\n",
    "\n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1\n",
    "\n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "\n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word,1),0)\n",
    "\n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word,0),0)\n",
    "\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing, we are importing the sample tweets from nltk and splitting\n",
    "the data into training sets and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import dataset from nltk\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "#create labels\n",
    "one_array = [1] * len(all_positive_tweets)\n",
    "zero_array = [0] * len(all_negative_tweets)\n",
    "\n",
    "all_tweets = all_positive_tweets + all_negative_tweets\n",
    "all_labels = one_array + zero_array\n",
    "tweets_and_labels = np.vstack((all_tweets,all_labels)).T\n",
    "\n",
    "#extract X (data) and y (labels) columns\n",
    "nltk_X = tweets_and_labels[:,0].tolist()\n",
    "nltk_y = tweets_and_labels[:,1].T.astype(int).tolist()\n",
    "\n",
    "#split the data set into train and test sets with a 80/20 ratio\n",
    "nltk_X_train, nltk_X_test, nltk_y_train, nltk_y_test = sklearn.model_selection.train_test_split(nltk_X, nltk_y, test_size=0.20, random_state=0)\n",
    "nltk_y_train = np.array([nltk_y_train]).T\n",
    "nltk_y_test = np.array([nltk_y_test]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With our required functions for processing tweets ready to go, we can begin\n",
    "to build our logistic regression model.\n",
    "\n",
    "The logistic regression maps the input $\\mathbf{x}_i$ into the following\n",
    "output:\n",
    "\n",
    "$p(y_i = 1 \\mid \\mathbf{x}_i, \\mathbf{w}) = \\sigma(\\mathbf{w}^T\\mathbf{x}_i) =  \\sigma(w_0 + w_1 x_1 + w_2 x_2)$.\n",
    "\n",
    "$\\sigma$ is the sigmoid function, that is defined as:\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1 + e^{-z}} = (1+e^{-z})^{-1}$\n",
    "\n",
    "The output of the sigmoid function is a value between 0 and 1. Let us\n",
    "define the sigmoid function to be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Returns the element-wise logistic sigmoid of z.\"\"\"\n",
    "    # Your code here. Aim for 1 line.\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Cost Function and Gradient Descent\n",
    "\n",
    "Our goal is to find a configuration of our parameters $\\mathbf{w}$ that\n",
    "minimizes our objective function (BCE). For logistic regression, we use\n",
    "the binary cross-entropy loss. Recall from Lecture 1 that the basic\n",
    "logistic regression training objective (learning objective) is:\n",
    "\n",
    "$$\n",
    "\\ell_\\text{LR}(\\mathbf{w}) = \\sum_{i=1}^N y_i \\ln \\sigma(\\mathbf{w}^T \\mathbf{x}_i) + (1-y_i) \\ln \\left(1-\\sigma(\\mathbf{w}^T \\mathbf{x}_i)\\right)\n",
    "$$\n",
    "\n",
    "The \"basic\" gradient for the above training objective is on a slide\n",
    "titled \"Maximum likelihood estimate for LR\" from Lecture 1, and\n",
    "reproduced here:\n",
    "\n",
    "$$\n",
    "\\nabla \\ell_\\text{LR}(\\mathbf{w}) = \\sum_{i=1}^N (\\sigma(\\mathbf{w}^T \\mathbf{x}_i) - y_i)\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "Let's define a few functions that implement these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w, lr, num_iters):\n",
    "\n",
    "    m = len(x)\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "\n",
    "        z = np.dot(x,w)\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        # calculate the cost function\n",
    "        J = (-1/m)*(np.dot(y.T,np.log(h)) + np.dot((1-y).T,np.log(1-h)))\n",
    "\n",
    "        # update the weights by gradient descent\n",
    "        w = w - (lr/m)*np.dot(x.T, h-y)\n",
    "\n",
    "    J = float(J)\n",
    "    return J, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4 Training and Testing our Model\n",
    "\n",
    "As all the required functions are ready we can finally train our model\n",
    "using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost function at the end of training is 0.24071649.\n",
      "The resulting vector of weights is [7e-08, 0.00052523, -0.00055603]\n"
     ]
    }
   ],
   "source": [
    "nltk_freqs = build_freqs(nltk_X_train, nltk_y_train)\n",
    "\n",
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "nltk_X_gd = np.zeros((len(nltk_X_train), 3))\n",
    "for i in range(len(nltk_X_train)):\n",
    "    nltk_X_gd[i, :]= extract_features(nltk_X_train[i], nltk_freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "nltk_Y_gd = nltk_y_train\n",
    "\n",
    "# Apply gradient descent to extract the weight vector w\n",
    "nltk_J, nltk_w = gradient_descent(nltk_X_gd, nltk_Y_gd, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {nltk_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(nltk_w)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Right above here, we receive and print the final cost J as well as the\n",
    "weight matrix w after all the training is done. This weight matrix\n",
    "constitutes our binary classification based on logistic regression model.\n",
    "\n",
    "Let's proceed to write two more functions which when given a tweet, will\n",
    "predict results using the freqs dictionary and weights matrix. The second\n",
    "function will use the predict function and provide the accuracy of\n",
    "the model on the given testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        freqs: a dictionary of the frequencies of each tuple (word, label)\n",
    "        theta: vector of weights\n",
    "    Output:\n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    \"\"\"\n",
    "\n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet, freqs)\n",
    "\n",
    "    # make the prediction using x and theta\n",
    "    z = np.dot(x,theta)\n",
    "    y_pred = sigmoid(z)\n",
    "    return y_pred\n",
    "\n",
    "def test_logistic_regression(X_test, y_test, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X_test: a list of tweets\n",
    "        y_test: vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary of the frequencies of each tuple (word, label)\n",
    "        theta: vector of weights\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "\n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "\n",
    "    for tweet in X_test:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "\n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "    # With the above implementation, y_hat is a list, but test_y is a\n",
    "    # (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them\n",
    "    # using the '==' operator\n",
    "    y_hat = np.array(y_hat)\n",
    "    test_y = y_test.reshape(-1)\n",
    "    accuracy = np.sum((test_y == y_hat).astype(int))/len(X_test)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With all the required functions defined, we can proceed to try out our\n",
    "model and look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 99.35000000000001%\n"
     ]
    }
   ],
   "source": [
    "acc = test_logistic_regression(nltk_X_test, nltk_y_test, nltk_freqs, nltk_w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have obtained a pretty high accuracy (around 99.0%) with the trained\n",
    "model. Let's formulate a few sentences and see what the model predicts\n",
    "out of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tweet is negative\n",
      "This tweet is positive\n",
      "This tweet is positive\n"
     ]
    }
   ],
   "source": [
    "tweet = \"I hate insta so much everyone is nicer on here\"\n",
    "prediction = predict_tweet(tweet, nltk_freqs, nltk_w)\n",
    "print(\"This tweet is positive\" if prediction >= 0.5 else \"This tweet is negative\")\n",
    "\n",
    "tweet = \"Everyday is my favorite, waking up one more day makes each day awesome\"\n",
    "prediction = predict_tweet(tweet, nltk_freqs, nltk_w)\n",
    "print(\"This tweet is positive\" if prediction >= 0.5 else \"This tweet is negative\")\n",
    "\n",
    "tweet = \"Checking the box full of ole' photos is always fun!\"\n",
    "prediction = predict_tweet(tweet, nltk_freqs, nltk_w)\n",
    "print(\"This tweet is positive\" if prediction >= 0.5 else \"This tweet is negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Expanding the data set\n",
    "\n",
    "Let's continue exploring the model by training it using another popular data\n",
    "set. The data set in question is \"Twitter US Airline Sentiment\" (retrieved\n",
    "from https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment?resource=download ).\n",
    "For the ease of use we have included the data set already\n",
    "within the repository. Let's do a few lines of code to import and preview\n",
    "the data in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Tweets.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Unlike our previous data set, this one contains more columns which we might\n",
    "not need. It is our job to process the data to make it fit the model\n",
    "defined above. However, do observe that there is one more sentiment in the\n",
    "labels: the neutral label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: airline_sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAH2CAYAAADAnqDaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAArmklEQVR4nO3debhkVX3u8e8rjYoDAtIoAtpocAASMbQIGhUliSR6xRgHjAMOuTiLxsSrxlxJDAnRJA7caCQOwBUvEpxwljQ2RhygEZRZUVEQlEFRcGgFfvePvY4U1XW6Tw91zuLw/TxPPbVr7WntXavqvGfvtXelqpAkSVJ/brPQFZAkSdJkBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUdKuS5KgklWTZQtdlWhbjNia5e5Kjk1ya5Ia2fVstdL02lcX4nvVksbcfLW4GNW209qW3Po9nr8eyL05y8fRqr1uIo4BnAqcAfw/8LfDLhazQ+khyaGv7+y50XaYpybPX9zM+T45iPdtP246VU6/ZFC2GbRAsWegKaFH42wllLwfuArwVuGZs3FnTrY4WkyS3Bf4A+K+qevpC12dKXgMcDnx/oSuy2NxK2o8WMYOaNlpVHTpe1v6jvgvwlqq6eJ6rpMXl7gxH/y9b6IpMS1VdDly+0PVYpBZ9+9Hi5qlPzbskT0ny+SQ/SfKLJGcneU2S241Ms2+SAu4F3Gvs1OlRI9M9Icn7knwjyc+SXJfkjCQvS7LR7TvJyrbOJUlem+SbSVYnuSTJP7X/1kenXzZex0nLGyvbt81zaJLlST7d9s2Pk3wwyU5tunsnOS7JlW2/fS7JA9dS/dsk+YskFyT5Zeuf8+YkW85Stx2T/J8k327beHWSE5M8eMK0vzmVl+TPknyl7fuL17FLZ+bfJckxSb6f5FdJLmuvdxmb7mLgu+3lQZPawFrW8fAkH2vbvTrJD5J8OcnrJ0x7h9YGzxppR19K8rQJ046+X3sk+USSa5L8PMkpSR46YRtm1vm50bY8Ms0afdRG21KS+yQ5ob0n1yb5bJLd23RLkxyZ5PL2Pp+e5FGz7JMlSV7U9sNPW53PTPKS8c/L2PqXtbZ3VVvHqiSPG5t+JfDe9vK9uflndlmb5s5J/ibJOW391yb5VpIPJNlz8js5cTum1n7STt+2l48c245Dk9yprfPUsfm2aPumkjxzbNyLWvlzx8q3SfKPSc7P8Jn+SZIVSf5wLfV7WobP/o/b+s5P8rrc/PtzrdswMt3j2/oub5+Ry1obftFs69f884ia5lWSf2A4zXMV8H7gOuCPgH8AHpPkD6rq18DFDKdUX95mfcvIYs4aGT4cuBH4CsNpo7sAj2Y45fpghn4pm8L7gYcDnwJ+Cvwx8CpgO+A5m2gdDwb+F0M/mv8Afht4IvDbSR4PfAG4ADiGIcA+ETgpyb2r6roJy3sz8AjgeOCjwGMY9ufDk/xeVf2mj06S3wU+C2wDfAb4ELAt8ATgC0n+pKo+OWEdr2Q4rfQx4HMM+3+tMgS//wLuDJwInAfcH3g6cECS/apqVZv8LcAy4BDga8BHWvlZ61jH/sAnGN6rExnaxjbAA4AXMXK6PkOn8pOBBwFfBd7D8E/sY4D3J9mtql43YTXLGdrAl4B3AfcE/hRYkWSPqrpwZBueADwSOJqhba+PZQzt+3yGvlbLgD8BVibZB/h0284PtG08EPhUkvtW1fdGtnNzhvfpMcCFDG36l8CjgCOAhzD583Iv4DTg28D/bet4KvDRJL9fVZ9r0x3F0M3hAIb2dtbIMq5JklbXh3LTPrse2AnYF/hv4Ix17Yx5aD9nMbSP1zOEvKNGxq2squuSnAY8JMmdq+raNu5hwExY2o9hX814dHteMbId9wJWtvr9N8O+uSPwOODTSZ5fVf8xtu3vBp4LXMrwGb0G2Bt4A7Bf+/68fl3b0JZ1MPBO4AcM7eIqhu+z32H4Tnv7WvaR5lNV+fCxyR8Mf4wKWDZStk8r+x5w95HyJQxfFAW8dsJyLl7Leu4zoew2DH8QC3jI2Lijxuu1ju1Y2aY/A9hmpPyOwEXADWPbsqxNf9TaljdWtm+bp4Cnj417dyv/EfDXY+P+po07ZJZtvAq419h++WAb9zdj+/8ihj/ajxxb1j0YQs7lwO1Gyg9ty/kZ8KD1aBdhCByTtvWprfwC4DZz3aezrGdmOx84Ydy2s+yvV42V357hj+eNwB6zvF/PHpvn+a387WPlM/tr31nqu0a7HNnuWst7/yPg38f21zPbuDfPUocjgM1GyjcbaWcHzLL+148t6zGt/JNj5c+etF/auN9u4z48y2d2617aT5uvGILZpHF/18Y/dqTsHxmC58nAJWPbdhXwrbFlrGxt68Cx8q0YgtYvgLtN2LcfAraY5b09ZD224QxgNbDduj4jPhb24alPzaeZw/5/X1U/mCms4T/AVzJ8af35+iywqr41oexGhiNqMPxB2RT+V1X9aGQdPwOOZfgSXr6J1vGFqjp2rOzo9vwThqOHo45pz3vMsry3VtXMaZ+Z/fJXDPt59BTMY4H7AEdU1SmjC6iqy4A3MvTz2W/COo6sqjNnWf8kD2U4+vGl8W2tqg8wHDW8H/B767HMtfnFeEFVXTUznOSuwDOAVVX1xrHpfslwhDPAn01Y9qlVddRY2XsY/ljvtXHVvpmLWfO9n2kXtwP+qr23M97f6rDHTEE7rfkShqMnr6iqG2bGteFX0sLPhPV/l+FKSUbm+QzDP1wbsp2T3pMbq+rHc5h3vtvPbGaOjI1+JvZjCD8fBHZMct9WvgdwV25+NO2BDEdYP1hVx40uuKquYTgSdnuGI7QzDmF4X59bVeP78A3A1Ux+/9bmeuDX44WjnxEtPE99aj79bns+eXxEVX0jyaXAzkm2al9W69T+0P4Vw6nIezMc6Rq1w4ZX92ZWTSi7pD1vPcV1zHSAPmv0j2szc4XgjrMs75Txgqr6dpJLgGUj+3mfNvpeo/1XRsz0+3kAMH7687RZ1j2bWdvASPnvMZyG/Px6LnvUsQynhr+S5AMMp2VPrapLx6Z7MMMRpZpl2zdvzw+YMG6N96uqfp3kh2y6NgGT3/uZdvGNuunU20wdbmh1GG0X92UIC98EXjechVzDL5i8nZPWD0P732dC+WzOYzhS9LR22u+jDMFqVVX9ao7LmK/2sy5fYthf+wEkuUur2xtH6rYf8A1uOu05WueZ/XaXWdrd0vb8gLb8OwAPZDgy9/JZ3r/VTH7/ZnMs8C/Aue0zcgrDZ+TK9ViG5oFBTfNppv/SbFe3Xc7Qz+curHlLjzW0vkWnAzszBIZjGE4FXc9w+uAQbuozslFmCY7Xt+fNNsU6GI6azbaONcZV1fXtC3vz8XHND2cp/wFDv6OZ/XzXVv7kddTvTrMsa33MpQ3A8P5tsKr6UOvs/kqGo4fPB0hyBvCaqjqpTTqz7Q9uj9lM2vZrZpn2ejZdm4C1v/eT2sxMHUbbxcx27sJNFzZMsr7bOeezMi1APhr438CTgH9qo65NcjTD+zKpr+WoeWk/61JVv0ryBeD3k2zHELw2A1ZU1flJLmMIau9oz8XNg9rM+/EH7TGbmfdja4Yju0tZ+/u3Ptvwr0muYuiz+TKG/quV5BSGo7ST/nHUAvDUp+bTzB+Vu88yfvux6dblzxlC2t9W1UOq6kVV9boabhfygQ2v5kaZOQU12z9BW81TPQDuNkv5zP7/ydjzAVWVtTwm3S+v1rNOm7oNzKqqPlFVj2b4I7cfw8UVuwEfT7Lr2HrevI5tn3gV5S3IzHZ+eB3bufM0K1FVP66qV1TVTgyh8c8Z+pS9hCHUrMu8tZ85OJkhPD2aoX2tBmauBP0c8Kh2JebDgXOr6oqReWfqd8g63o/njE1/5jqmn3iobTZVdUxV7c0QHB/L0FfxEcBnWgBVBwxqmk8zfZn2HR+R5LcYTtV8Z+zo1Q3MfnTit9rzByeMe+SGVXGjzfSz2Wl8RIbbYtx3vHyK1tgHSe7NULeLR/bzl9vzw+ehTrO2gbHyr26qFVbVz6rq5Kr6C4ari2/LcKUxDEdib2T62z5z6nBTHmlbHxfQrhBsV39Oy5y3s6ouqqp3M7TT6xiuFl2X+Ww/N7L27Rjtp/ZohtOGvxwZtw3wQobuGCvG5l2vz1w70ngusFuSbeYyT7OubZhZ/jVV9cmq+p8MF7ZsM9e6afoMappP72nPr0sy0weDJJsB/8zQHt89Ns/VwNIkW0xY3sXted/RwiQPYrgFyLxr/YUuAB42ctRmZhv/FZi0HdNySOsLNFOH2wBvYtjP7x2Z7qPAt4AXJ/njSQtKsk/rJ7OxTmW4NcTvJXnS2DqexPDf/DcY+i5tsCT7zdJmZo4y/hygHeU4Flie4f5eaxwJzXAPs4090nR1e77nRi5ng7QLdo5gOOL0tkn7Jsn2o212A826nUl2TrLbhHm2ZuiisMZFBhPMS/tprmbCP1wjzuCm25Hsxs3D2MzwzPfQzfrUtdOK/w08MWP3VpuR5LfHjmr9K8M/Ge/JhN8pTbJ1u83OnLYhyf6T2jvDLTqgfUa08OyjpnlTVV9M8kaGe0+dk+QEhts7/BGwO8OX65vGZlvB0Hfo00k+z3B64WtV9TGGPml/Bbwlww0+v8lwOuVxDJewP3X6WzXRmxgC56lJ/pOb7lW1OcO9nB44T/U4FTirdRT+CcMVsA9k+APzmyscWwf4JzLcP+0TSb7I0On75wxf8g9muFBjezbyy7uqKslBwEnAB5J8lCHY3o/hXmPXAs8au4pxQ/wLwwUTKxkC/a+APRmOfHwXGL3S7iUM7ebvgGe2vkc/ZLg1yQMYtv9pwHc2oj6fYzi68Y8ZblT7Y4Cq+vu1zrVpvYHh/X8B8D+SnMxwQcp2DNv/MOCvGTr9b6gvMbSRl7cjPzP9JI9o6/5w6yd4DsMFEUsZgs7m3NRnbVbz2H5g+O45MMnHGD4z1wOfr6rPt7rc2PpzHTAy/Uw9v5fkWwxXU9/AhAt7GK4kPhl4d5KXMdwr7xqGMwu/w/CduA9wRVvmezLcFPhFwLeSzFx5uw1DF5BHMPwD9oI5bsNxwC9be7+Y4TTuwxna+xkM96pTD6qDe4T4WHwPJtxHbWTcgQyh7FqGEHMuwx+I20+Y9o4MfVcuZfiSudn9kIBdGW56eQVD6DuDod/LsvFp2/RHzVavWbZjJWP3PRsZ92xmv2fU89p2rWbocP9Ohn4gayyPm+7LdeiE5UzcjpHxa9wnaWQb783Qmf6Ctp+/z3AD0C1nWdZ2DLeBOIfhj+11DOH3BIZbWCwZmfZQ1nJfsDns1/sx3BD0cobbA1wOvA+43/rug1mW/xTg/7X6X8dwQ9hzgMOApROmvy1DYPsiQ6hdzfBHcAVDJ+u7zuX9Gmn7F08ofwY33R+rRtvBpHa5Ie/9HOoQhvusrWC48OZXrV18AXgtsNN6rH+NttzK92cIbNfNbGdb1o4Mp55PZfhMrGb4XH8K+KOe2s/I5+H9DGHzhknvOfDSVv4TRu5N18a9s437ylrWcee2389o++sXDP8QfAI4GLjjhHkeB3yc4TvvV21fnsZwC5X7z3UbGALdhxluZPzz1h7OZPhH+s4b8rn2MZ1H2hsmSZKkzthHTZIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTi/Y+attuu20tW7ZsoashSZK0TmecccZVVbV0vHzRBrVly5axapW/KStJkvqX5LuTyj31KUmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdWrLQFZAkaT6885vHLXQVtMg8f5cDp74Oj6hJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdmmpQS/KKJOcmOSfJ/0ty+yTbJDkpyTfb89Yj078myUVJLkzymJHyPZOc3ca9LUmmWW9JkqQeTC2oJdkBeBmwvKp2BzYDDgReDayoql2AFe01SXZt43cD9gfenmSztrh3AAcDu7TH/tOqtyRJUi+mfepzCbBFkiXAHYDLgAOAo9v4o4EntOEDgOOqanVVfQe4CNgryfbAllX1paoq4JiReSRJkhatqQW1qvo+8M/A94DLgZ9U1WeBu1XV5W2ay4Ht2iw7AJeMLOLSVrZDGx4vlyRJWtSmeepza4ajZDsD9wDumOQZa5tlQlmtpXzSOg9OsirJqiuvvHJ9qyxJktSVaZ76/H3gO1V1ZVX9GvgQ8FDgh+10Ju35ijb9pcBOI/PvyHCq9NI2PF6+hqo6sqqWV9XypUuXbtKNkSRJmm/TDGrfA/ZOcod2leZ+wPnAicBBbZqDgI+24ROBA5PcLsnODBcNnNZOj16bZO+2nGeNzCNJkrRoLZnWgqvqK0lOAL4KXA+cCRwJ3Ak4PsnzGMLck9v05yY5HjivTf/iqrqhLe6FwFHAFsCn2kOSJGlRm1pQA6iq1wOvHytezXB0bdL0hwGHTShfBey+ySsoSZLUMX+ZQJIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpU1MNakm2SnJCkguSnJ9knyTbJDkpyTfb89Yj078myUVJLkzymJHyPZOc3ca9LUmmWW9JkqQeTPuI2luBT1fV/YEHAucDrwZWVNUuwIr2miS7AgcCuwH7A29PsllbzjuAg4Fd2mP/KddbkiRpwU0tqCXZEngE8G6AqvpVVV0DHAAc3SY7GnhCGz4AOK6qVlfVd4CLgL2SbA9sWVVfqqoCjhmZR5IkadGa5hG1ewNXAu9NcmaSdyW5I3C3qrocoD1v16bfAbhkZP5LW9kObXi8XJIkaVGbZlBbAvwu8I6qehDwM9ppzllM6ndWaylfcwHJwUlWJVl15ZVXrm99JUmSujLNoHYpcGlVfaW9PoEhuP2wnc6kPV8xMv1OI/PvCFzWynecUL6GqjqyqpZX1fKlS5dusg2RJElaCFMLalX1A+CSJPdrRfsB5wEnAge1soOAj7bhE4EDk9wuyc4MFw2c1k6PXptk73a157NG5pEkSVq0lkx5+S8Fjk1yW+DbwHMYwuHxSZ4HfA94MkBVnZvkeIYwdz3w4qq6oS3nhcBRwBbAp9pDkiRpUZtqUKuqs4DlE0btN8v0hwGHTShfBey+SSsnSZLUOX+ZQJIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVNzCmpJVsylTJIkSZvOkrWNTHJ74A7Atkm2BtJGbQncY8p1kyRJulVba1ADng+8nCGUncFNQe2nwL9Nr1qSJElaa1CrqrcCb03y0qo6Yp7qJEmSJNZ9RA2AqjoiyUOBZaPzVNUxU6qXJEnSrd6cglqS/wvcBzgLuKEVF2BQkyRJmpI5BTVgObBrVdU0KyNJkqSbzPU+aucAd59mRSRJknRzcz2iti1wXpLTgNUzhVX1+KnUSpIkSXMOaodOsxKSJEla01yv+jxl2hWRJEnSzc31qs9rGa7yBLgtsDnws6racloVkyRJurWb6xG1O4++TvIEYK9pVEiSJEmDuV71eTNV9RHg0Zu2KpIkSRo111OfTxx5eRuG+6p5TzVJkqQpmutVn/9jZPh64GLggE1eG0mSJP3GXPuoPWfaFZEkSdLNzamPWpIdk3w4yRVJfpjkg0l2nHblJEmSbs3mejHBe4ETgXsAOwAfa2WSJEmakrkGtaVV9d6qur49jgKWTrFekiRJt3pzDWpXJXlGks3a4xnA1dOsmCRJ0q3dXIPac4GnAD8ALgeeBHiBgSRJ0hTN9fYcbwAOqqofAyTZBvhnhgAnSZKkKZjrEbXfmQlpAFX1I+BB06mSJEmSYO5B7TZJtp550Y6ozfVonCRJkjbAXMPWvwBfTHICw09HPQU4bGq1kiRJ0px/meCYJKsYfog9wBOr6ryp1kySJOlWbs6nL1swM5xJkiTNk7n2UZMkSdI8M6hJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnZp6UEuyWZIzk3y8vd4myUlJvtmetx6Z9jVJLkpyYZLHjJTvmeTsNu5tSTLtekuSJC20+Tiidghw/sjrVwMrqmoXYEV7TZJdgQOB3YD9gbcn2azN8w7gYGCX9th/HuotSZK0oKYa1JLsCDwWeNdI8QHA0W34aOAJI+XHVdXqqvoOcBGwV5LtgS2r6ktVVcAxI/NIkiQtWtM+ovYW4FXAjSNld6uqywHa83atfAfgkpHpLm1lO7Th8fI1JDk4yaokq6688spNsgGSJEkLZWpBLcnjgCuq6oy5zjKhrNZSvmZh1ZFVtbyqli9dunSOq5UkSerTkiku+2HA45P8MXB7YMsk7wN+mGT7qrq8nda8ok1/KbDTyPw7Ape18h0nlEuSJC1qUzuiVlWvqaodq2oZw0UCJ1fVM4ATgYPaZAcBH23DJwIHJrldkp0ZLho4rZ0evTbJ3u1qz2eNzCNJkrRoTfOI2mwOB45P8jzge8CTAarq3CTHA+cB1wMvrqob2jwvBI4CtgA+1R6SJEmL2rwEtapaCaxsw1cD+80y3WHAYRPKVwG7T6+GkiRJ/fGXCSRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6ZVCTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVNLFroCvTvhtCsXugpaZJ6019KFroIk6RbCI2qSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqeWLHQFJC28az59xEJXQYvMVvu/dKGrIC0KHlGTJEnqlEFNkiSpUwY1SZKkThnUJEmSOmVQkyRJ6pRBTZIkqVMGNUmSpE4Z1CRJkjplUJMkSeqUQU2SJKlTBjVJkqROGdQkSZI6NbWglmSnJJ9Lcn6Sc5Mc0sq3SXJSkm+2561H5nlNkouSXJjkMSPleyY5u417W5JMq96SJEm9mOYRteuBV1bVA4C9gRcn2RV4NbCiqnYBVrTXtHEHArsB+wNvT7JZW9Y7gIOBXdpj/ynWW5IkqQtTC2pVdXlVfbUNXwucD+wAHAAc3SY7GnhCGz4AOK6qVlfVd4CLgL2SbA9sWVVfqqoCjhmZR5IkadGalz5qSZYBDwK+Atytqi6HIcwB27XJdgAuGZnt0la2QxseL5ckSVrUph7UktwJ+CDw8qr66domnVBWaymftK6Dk6xKsurKK69c/8pKkiR1ZKpBLcnmDCHt2Kr6UCv+YTudSXu+opVfCuw0MvuOwGWtfMcJ5WuoqiOranlVLV+6dOmm2xBJkqQFMM2rPgO8Gzi/qv51ZNSJwEFt+CDgoyPlBya5XZKdGS4aOK2dHr02yd5tmc8amUeSJGnRWjLFZT8MeCZwdpKzWtlrgcOB45M8D/ge8GSAqjo3yfHAeQxXjL64qm5o870QOArYAvhUe0iSJC1qUwtqVfUFJvcvA9hvlnkOAw6bUL4K2H3T1U6SJKl//jKBJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR1yqAmSZLUKYOaJElSpwxqkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapIkSZ0yqEmSJHXKoCZJktQpg5okSVKnbjFBLcn+SS5MclGSVy90fSRJkqbtFhHUkmwG/BvwR8CuwNOS7LqwtZIkSZquW0RQA/YCLqqqb1fVr4DjgAMWuE6SJElTdUsJajsAl4y8vrSVSZIkLVpLFroCc5QJZbXGRMnBwMHt5XVJLpxqrTRuW+Cqha6ENGW28zl52UJXQBvHdj4HL+Bpm3Jx95pUeEsJapcCO4283hG4bHyiqjoSOHK+KqWbS7KqqpYvdD2kabKd69bAdt6PW8qpz9OBXZLsnOS2wIHAiQtcJ0mSpKm6RRxRq6rrk7wE+AywGfCeqjp3gaslSZI0VbeIoAZQVZ8EPrnQ9dBaedpZtwa2c90a2M47kao1+uRLkiSpA7eUPmqSJEm3OgY1bVJJliX5sw2c97pNXR9pU0nygiTPasPPTnKPkXHv8tdStFgl2SrJi0Ze3yPJCQtZp1sTT31qk0qyL/CXVfW4CeOWVNX1a5n3uqq60xSrJ20SSVYytPNVC10XadqSLAM+XlW7L3Rdbo08oibgN0fCzk/yH0nOTfLZJFskuU+STyc5I8l/J7l/m/6oJE8amX/maNjhwMOTnJXkFe3Iw38m+Rjw2SR3SrIiyVeTnJ3EnwLT1LX2fUGSo5N8PckJSe6QZL8kZ7a2+J4kt2vTH57kvDbtP7eyQ5P8ZWv3y4FjWzvfIsnKJMuTvDDJG0fW++wkR7ThZyQ5rc3zzvYbxtJG24Dv7/sk+XKS05P83cz391q+nw8H7tPa7pva+s5p83wlyW4jdVmZZM8kd2yfqdPbZ8zv+g1VVT58ACwDrgf2aK+PB54BrAB2aWUPAU5uw0cBTxqZ/7r2vC/Df14z5c9muGHxNu31EmDLNrwtcBE3Hdm9bqH3g4/F+Wjtu4CHtdfvAV7H8NN0921lxwAvB7YBLhxpl1u150MZjqIBrASWjyx/JUN4W8rwu8Qz5Z8Cfg94APAxYPNW/nbgWQu9X3wsjscGfH9/HHhaG37ByPf3xO/ntvxzxtZ3Tht+BfC3bXh74Btt+B+AZ7ThrYBvAHdc6H11S3x4RE2jvlNVZ7XhMxg+jA8F/jPJWcA7GT6I6+ukqvpRGw7wD0m+DvwXw2+23m0j6izN1SVVdWobfh+wH0Ob/0YrOxp4BPBT4JfAu5I8Efj5XFdQVVcC306yd5K7AvcDTm3r2hM4vX2W9gPuvfGbJP3G+nx/7wP8Zxt+/8gyNuT7+XjgyW34KSPL/UPg1W3dK4HbA/dcv00S3ILuo6Z5sXpk+AaGD+g1VbXHhGmvp506TxLgtmtZ7s9Ghp/OcNRhz6r6dZKLGT7A0rTNqUNuDTfY3oshTB0IvAR49Hqs5wMMf7AuAD5cVdU+I0dX1WvWs87SXK3P9/ds1vv7uaq+n+TqJL8DPBV4fhsV4E+ryt/c3kgeUdPa/BT4TpInwxDIkjywjbuY4QgBwAHA5m34WuDOa1nmXYAr2pfAo5jlR2ilKbhnkn3a8NMYjhgsS/JbreyZwClJ7gTcpYabbL8c2GPCstbWzj8EPKGt4wOtbAXwpCTbASTZJoltX9O0tu/vLwN/2oYPHJlntu/ndX2vHwe8iuFzc3Yr+wzw0vZPCkketLEbdGtlUNO6PB14XpKvAecyhDKA/wAemeQ0hr4PM0fNvg5cn+RrSV4xYXnHAsuTrGrLvmCqtZducj5wUDutsw3wZuA5DKeGzgZuBP6d4Q/Sx9t0pzD0wRl3FPDvMxcTjI6oqh8D5wH3qqrTWtl5DH3iPtuWexIb1o1AWh+zfX+/HPiL9v29PfCTVj7x+7mqrgZOTXJOkjdNWM8JDIHv+JGyNzD8A//1duHBGzblht2aeHsOSYtevL2A9BtJ7gD8op2WP5DhwgKvyuyUfdQkSbp12RP4P+205DXAcxe2Olobj6hJkiR1yj5qkiRJnTKoSZIkdcqgJkmS1CmDmiRJUqcMapK6leSTSbaaZdzFSbZtw1+c14rNUZLXjr2eaj2TbJXkRdNch6T55VWfkm5R2i0FAnyb4YfRr1rgKs0qyXVVdad5XN8yvF+ctKh4RE1SF5J8JMkZSc5NcnAruzjJtkmWJTk/yduBrwI7jc17XXveN8nKJCckuSDJsSM/YbNnklPaOj6TZNZfBkjysiTnJfl6kuNa2R2TvCfJ6UnOTHJAK392kg8l+XSSbyZ5Yys/HNii/XrBsRPqeUqS45N8I8nhSZ6e5LQkZye5T5tuaZIPtnWenuRhrfzQVpeVSb6d5GWt6ocD92nrnHQHeUm3MN7wVlIvnltVP2o/yXR6kg+Ojb8f8JyqehFAy1+TPAjYDbgMOBV4WJKvAEcAB1TVlUmeChzG7Df6fDWwc1WtHjn1+tfAyVX13FZ2WpL/auP2aOtdDVyY5IiqenWSl6zlR7EfCDwA+BHD0cF3VdVeSQ4BXsrwMz9vBd5cVV9Ick+G3098QJv//sCjGH7y6sIk72j13n09f4hbUscMapJ68bIkf9KGdwJ2GRv/3ar68hyWc1pVXQqQ5CxgGcPd13cHTmoBbzPg8rUs4+vAsUk+Anyklf0h8Pgkf9le3x64ZxteUVU/aes8j+HHrC9ZRz1Pr6rL2zzfAj7bys9mCGAAvw/sOhJKt0wy8+PYn6iq1cDqJFcAd1vH+iTdAhnUJC24JPsyhJJ9qurnSVYyBKFRP5vj4laPDN/A8D0X4Nyq2meOy3gs8Ajg8cDfJNmtLeNPq+rCsbo/ZJZ1rk89bxx5fePI/Ldh2Ce/GFvn+PxzXaekWxj7qEnqwV2AH7eQdn9g7028/AuBpUn2AUiyeQtfa0hyG2Cnqvoc8CpgK+BODKcdXzrS5+1Bc1jvr5NsvhH1/izwkpG67bGO6a9lOBUqaZEwqEnqwaeBJUm+DrwBmMspzjmrql8BTwL+KcnXgLOAh84y+WbA+5KcDZzJ0EfsmlavzYGvJzmnvV6XI9v0x25g1V8GLG8XNZwHvGBtE1fV1cCpSc7xYgJpcfD2HJIkSZ3yiJokSVKn7Hwq6VYryb8BDxsrfmtVvXch6iNJ4zz1KUmS1ClPfUqSJHXKoCZJktQpg5okSVKnDGqSJEmdMqhJkiR16v8DmahY3Mh11QAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.airline_sentiment.value_counts())\n",
    "plt.figure(figsize = (10, 8))\n",
    "ax = sns.countplot(x = 'airline_sentiment', data = df, palette = 'pastel')\n",
    "ax.set_title(label = 'Total number of sentiments of tweets', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's define a few functions that will help us with the text processing.\n",
    "Remember, currently we are working with binary classification of positive\n",
    "and negative sentiments. For this reason, we will be discarding all neutral\n",
    "labels and their corresponding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    9178\n",
      "positive    2363\n",
      "Name: airline_sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#remove all neutral-labeled data\n",
    "df =  df[df.airline_sentiment != \"neutral\"]\n",
    "print(df.airline_sentiment.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we should convert the sentiments into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        1\n",
      "3        0\n",
      "4        0\n",
      "5        0\n",
      "6        1\n",
      "        ..\n",
      "14633    0\n",
      "14634    0\n",
      "14635    1\n",
      "14636    0\n",
      "14638    0\n",
      "Name: airline_sentiment, Length: 11541, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert sentiments to 0 or 1\n",
    "def convert_sentiment(sentiment):\n",
    "    if  sentiment == \"positive\":\n",
    "        return 1\n",
    "    elif sentiment == \"negative\":\n",
    "        return 0\n",
    "\n",
    "df.airline_sentiment = df.airline_sentiment.apply(lambda x : convert_sentiment(x))\n",
    "print(df.airline_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One final step, we will extract each tweet text from the data and their\n",
    "respective labels. We also need to split the data into training and test\n",
    "sets as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#extract columns in the csv file as rows\n",
    "airline_X = df['text'].tolist()\n",
    "airline_y = df['airline_sentiment'].tolist()\n",
    "\n",
    "#split into 80:20 train to test ratio\n",
    "airline_X_train, airline_X_test, airline_y_train, airline_y_test = sklearn.model_selection.train_test_split(airline_X, airline_y, test_size=0.20, random_state=0)\n",
    "airline_y_train = np.array([airline_y_train]).T.astype(float)\n",
    "airline_y_test = np.array([airline_y_test]).T.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data has now undergone transformation and is ready to be handled by\n",
    "our previously defined functions. We simply have to repeat the code\n",
    "sections written out before but now using the airline data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost function at the end of training is 0.48646059.\n",
      "The resulting vector of weights is [-2e-07, 1.937e-05, -0.00045547]\n",
      "The accuracy on the test set is 79.64486790818536%\n"
     ]
    }
   ],
   "source": [
    "airline_freqs = build_freqs(airline_X_train, airline_y_train)\n",
    "\n",
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "airline_X_gd = np.zeros((len(airline_X_train), 3))\n",
    "for i in range(len(airline_X_train)):\n",
    "    airline_X_gd[i, :]= extract_features(airline_X_train[i], airline_freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "airline_Y_gd = airline_y_train\n",
    "\n",
    "# Apply gradient descent to extract the weight vector w\n",
    "airline_J, airline_w = gradient_descent(airline_X_gd, airline_Y_gd, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {airline_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(airline_w)]}\")\n",
    "\n",
    "airline_acc = test_logistic_regression(airline_X_test, airline_y_test, airline_freqs, airline_w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(airline_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The resulting accuracy of the model trained on the airline sentiment\n",
    "data seems lower than the model trained on nltk's tweets sentiments.\n",
    "This is to be discussed in our report submission.\n",
    "\n",
    "### 1.7 Combining nltk's Dataset with Twitter US Airline Sentiment Analysis's Dataset\n",
    "\n",
    "As a final experiment, I would like to repeat the training and testing\n",
    "of our model, but this time with both data sets combined. Let's do a\n",
    "little processing of the data to achieve this objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#combine both list of tweets into one data set\n",
    "combined_X = airline_X + nltk_X\n",
    "combined_y = airline_y + nltk_y\n",
    "\n",
    "#split into 80:20 train to test ratio\n",
    "combined_X_train, combined_X_test, combined_y_train, combined_y_test = sklearn.model_selection.train_test_split(combined_X, combined_y, test_size=0.20, random_state=0)\n",
    "combined_y_train = np.array([combined_y_train]).T.astype(float)\n",
    "combined_y_test = np.array([combined_y_test]).T.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data is now ready to go through the training loops and testing outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost function at the end of training is 0.38524755.\n",
      "The resulting vector of weights is [-4e-08, 0.00036247, -0.0004816]\n",
      "The accuracy on the test set is 89.71919238802506%\n"
     ]
    }
   ],
   "source": [
    "combined_freqs = build_freqs(combined_X_train, combined_y_train)\n",
    "\n",
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "combined_X_gd = np.zeros((len(combined_X_train), 3))\n",
    "for i in range(len(combined_X_train)):\n",
    "    combined_X_gd[i, :]= extract_features(combined_X_train[i], combined_freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "combined_Y_gd = combined_y_train\n",
    "\n",
    "# Apply gradient descent to extract the weight vector w\n",
    "combined_J, combined_w = gradient_descent(combined_X_gd, combined_Y_gd, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {combined_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(combined_w)]}\")\n",
    "\n",
    "combined_acc = test_logistic_regression(combined_X_test, combined_y_test, combined_freqs, combined_w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(combined_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The accuracy on this model appears to be lower than that trained solely\n",
    "on nltk's sentiment data set, but shows to be higher than that trained\n",
    "on the airline sentiment data set. We will discuss this in detail in the\n",
    "written reports as well.\n",
    "\n",
    "### 1.8 Training the model by carrying over the weight vector\n",
    "\n",
    "In our previous three experiments, we conducted the training on\n",
    "a clean slate by initializing the weights vectors to zero. What\n",
    "would happen if we were to train our binary classification model\n",
    "on one data set and pass the resulting weight vector as the\n",
    "initial weight vector for the second data set? Let's take a\n",
    "look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost function at the end of training is 0.44955847.\n",
      "The resulting vector of weights is [-3e-08, 0.0005846, -0.00064061]\n"
     ]
    }
   ],
   "source": [
    "# Apply gradient descent to extract the weight vector w\n",
    "shared_w_J, shared_w_w = gradient_descent(airline_X_gd, airline_Y_gd, nltk_w, 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {shared_w_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(shared_w_w)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Because the model has already been trained on both data sets,\n",
    "we are testing it against the combined set's test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 90.80993269900209%\n"
     ]
    }
   ],
   "source": [
    "shared_w_acc = test_logistic_regression(combined_X_test, combined_y_test, combined_freqs, shared_w_w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(shared_w_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The results are close enough to that of the combined data set's model's\n",
    "results. As usual, we will discuss the significance of this output\n",
    "inside the written report.\n",
    "\n",
    "### 1.9 Introducing one more dataset: Sentiment140\n",
    "\n",
    "We will perform sections 1.6 to 1.10 again with an additional dataset.\n",
    "The sentiment140 dataset contains 1,600,000 tweets extracted using the\n",
    "Twitter API (retrieved from https://www.kaggle.com/datasets/kazanova/sentiment140 ).\n",
    "This means we are dealing with tweets once more! The tweets\n",
    "have been labeled (0 for negative, 2 for neutral, 4 for positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment140 = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)\n",
    "df_sentiment140.columns = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "print(df_sentiment140.shape)\n",
    "df_sentiment140.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This dataset is actually over 200 MB in file size and way too large to\n",
    "comfortably work with on a low-end computer. We will only work with\n",
    "about half of the data, shuffled with a constant random_state variable so\n",
    "that it is reproducible every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    250183\n",
       "0    249816\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment140 = shuffle(df_sentiment140,random_state=0)\n",
    "df_sentiment140 = df_sentiment140[1:200000]\n",
    "df_sentiment140['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the airline sentiment data set, this one contains some columns which\n",
    "we might not need. We will once again have to preprocess the data. The\n",
    "neutral label is also present and will have to be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    250183\n",
      "0    249816\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df_sentiment140.target.value_counts())\n",
    "plt.figure(figsize = (10, 8))\n",
    "ax = sns.countplot(x = 'target', data = df_sentiment140, palette = 'pastel')\n",
    "ax.set_title(label = 'Total number of sentiments of tweets', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    250183\n",
      "0    249816\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#remove all neutral-labeled data\n",
    "df_sentiment140 =  df_sentiment140[df_sentiment140.target != 2]\n",
    "print(df_sentiment140.target.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should convert the sentiments into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349381     0\n",
      "182051     0\n",
      "571236     0\n",
      "1339637    1\n",
      "758738     0\n",
      "          ..\n",
      "165784     0\n",
      "270676     0\n",
      "982023     1\n",
      "264632     0\n",
      "963253     1\n",
      "Name: target, Length: 499999, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert sentiments to 0 or 1\n",
    "def convert_sentiment_140(sentiment):\n",
    "    if  sentiment == 4:\n",
    "        return 1\n",
    "    elif sentiment == 0:\n",
    "        return 0\n",
    "\n",
    "df_sentiment140.target = df_sentiment140.target.apply(lambda x : convert_sentiment_140(x))\n",
    "print(df_sentiment140.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we need to extract the data and their corresponding labels. \n",
    "We also need to split the data into training and test sets as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#extract columns in the csv file as rows\n",
    "sentiment140_X = df_sentiment140['text'].tolist()\n",
    "sentiment140_y = df_sentiment140['target'].tolist()\n",
    "\n",
    "#split into 80:20 train to test ratio\n",
    "sentiment140_X_train, sentiment140_X_test, sentiment140_y_train, sentiment140_y_test = sklearn.model_selection.train_test_split(sentiment140_X, sentiment140_y, test_size=0.20, random_state=0)\n",
    "sentiment140_y_train = np.array([sentiment140_y_train]).T.astype(float)\n",
    "sentiment140_y_test = np.array([sentiment140_y_test]).T.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been successfully extracted and is now ready to be handled by\n",
    "the simple logistic regression model. We simply have to repeat the code\n",
    "sections written out before but now using the sentiment140 data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost function at the end of training is 0.60679866.\n",
      "The resulting vector of weights is [-1e-08, 0.00015862, -0.00013634]\n",
      "The accuracy on the test set is 69.065%\n"
     ]
    }
   ],
   "source": [
    "sentiment140_freqs = build_freqs(sentiment140_X_train, sentiment140_y_train)\n",
    "\n",
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "sentiment140_X_gd = np.zeros((len(sentiment140_X_train), 3))\n",
    "for i in range(len(sentiment140_X_train)):\n",
    "    sentiment140_X_gd[i, :]= extract_features(sentiment140_X_train[i], sentiment140_freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "sentiment140_Y_gd = sentiment140_y_train\n",
    "\n",
    "# Apply gradient descent to extract the weight vector w\n",
    "sentiment140_J, sentiment140_w = gradient_descent(sentiment140_X_gd, sentiment140_Y_gd, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {sentiment140_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(sentiment140_w)]}\")\n",
    "\n",
    "sentiment140_acc = test_logistic_regression(sentiment140_X_test, sentiment140_y_test, sentiment140_freqs, sentiment140_w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(sentiment140_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the results of the model trained on the sentiment140 dataset,\n",
    "we can observe that it is even lower than that of the airline sentiment\n",
    "dataset's! This is to be discussed in our report submission.\n",
    "\n",
    "### 1.10 Training the model using all three datasets\n",
    "\n",
    "One final experiment to end this section: we will carry over the weights\n",
    "obtained from training the model on sentiment140's dataset and continue\n",
    "the training with the combined dataset of nltk and airline sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost function at the end of training is 0.36469757.\n",
      "The resulting vector of weights is [-5e-08, 0.00045, -0.00051931]\n"
     ]
    }
   ],
   "source": [
    "# Apply gradient descent to extract the weight vector w\n",
    "reshared_w_J, reshared_w_w = gradient_descent(combined_X_gd, combined_Y_gd, sentiment140_w, 1e-9, 1500)\n",
    "print(f\"The cost function at the end of training is {reshared_w_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(reshared_w_w)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 80.69157577164076%\n"
     ]
    }
   ],
   "source": [
    "combined_freqs.update(sentiment140_freqs)\n",
    "\n",
    "reshared_w_acc = test_logistic_regression(combined_X_test, combined_y_test, combined_freqs, reshared_w_w)\n",
    "\n",
    "print(\"The accuracy on the test set is {}%\".format(reshared_w_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With one quick look at the accuracy, we can see that it has greatly\n",
    "improved. The reasons for this will be discussed within the report.\n",
    "This concludes all our experiments and the section for sentiment analysis\n",
    "by binary classification using logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Comp-472-Mini-Project)",
   "language": "python",
   "name": "pycharm-27aa6ddb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}